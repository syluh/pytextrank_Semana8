{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4878523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "0.2004709030170105 2\n",
      "[London, London]\n",
      "the National Health Service.as\n",
      "0.12475926473548551 2\n",
      "[the National Health Service.as, the National Health Service.as]\n",
      "father\n",
      "0.122463238391564 1\n",
      "[father]\n",
      "an insurance company\n",
      "0.08906737645471932 1\n",
      "[an insurance company]\n",
      "a nurse\n",
      "0.07520942813277859 1\n",
      "[a nurse]\n",
      "a flat\n",
      "0.07214600911087907 1\n",
      "[a flat]\n",
      "His mother\n",
      "0.0704878432291769 1\n",
      "[His mother]\n",
      "his mother\n",
      "0.0704878432291769 1\n",
      "[his mother]\n",
      "His father\n",
      "0.05656334692096224 1\n",
      "[His father]\n",
      "She\n",
      "0.0 1\n",
      "[She]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# Texto de Ejemplo\n",
    "text = \"His mother and father live in a flat in London. His father works in an insurance company, and his mother is a nurse. She works for the National Health Service.as.\"\n",
    "\n",
    "# cargar el modelo de SPacy\n",
    "nlp = nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Examinar el top de las frasdes rankeadas\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45732006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'textrank']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se importa las librerias pytextrank \n",
    "import pytextrank\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\");\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24f3dddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "                                                                                     \n",
      "6   textrank                                                              False      \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'textrank': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'textrank': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Se ve con mas detalle\n",
    "nlp.analyze_pipes(pretty=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b3dcfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shawn’s girlfriend’s name is Maria, and she’s Italian. She moved to the UK to study English a few years ago, and then stayed. She works in a café near Piccadilly Circus. \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from icecream import ic\n",
    "\n",
    "import pathlib\n",
    "\n",
    "text = pathlib.Path(\"/home/syluh/Escritorio/a.txt\").read_text()\n",
    "text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "586c021a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver la longitud del texto\n",
    "\n",
    "doc = nlp(text)\n",
    "len(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "270082b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tr.elapsed_time: 3.551006317138672\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ahora podemos acceder al componente PyTextRank dentro de la spaCycanalización y\n",
    "# usarlo para obtener más información para el procesamiento posterior del\n",
    "# documento. Por ejemplo, veamos cuál fue el tiempo transcurrido en \n",
    "# milisegundos para el procesamiento de TextRank :\n",
    "    \n",
    "tr = doc._.textrank\n",
    "ic(tr.elapsed_time);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d0a478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| phrase.rank: 0.1760412407399692\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'Italian'\n",
      "ic| phrase.chunks: [Italian]\n",
      "ic| phrase.rank: 0.17507452769776916\n",
      "    phrase.count: 2\n",
      "    phrase.text: 'Piccadilly Circus'\n",
      "ic| phrase.chunks: [Piccadilly Circus, Piccadilly Circus]\n",
      "ic| phrase.rank: 0.1182519975313125\n",
      "    phrase.count: 2\n",
      "    phrase.text: 'English'\n",
      "ic| phrase.chunks: [English, English]\n",
      "ic| phrase.rank: 0.1182519975313125\n",
      "    phrase.count: 2\n",
      "    phrase.text: 'Maria'\n",
      "ic| phrase.chunks: [Maria, Maria]\n",
      "ic| phrase.rank: 0.09214420443694846\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'UK'\n",
      "ic| phrase.chunks: [UK]\n",
      "ic| phrase.rank: 0.08133885040709285\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'Shawn'\n",
      "ic| phrase.chunks: [Shawn]\n",
      "ic| phrase.rank: 0.06939268322340339\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'a few years ago'\n",
      "ic| phrase.chunks: [a few years ago]\n",
      "ic| phrase.rank: 0.06094822033701628\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'Shawn’s girlfriend’s name'\n",
      "ic| phrase.chunks: [Shawn’s girlfriend’s name]\n",
      "ic| phrase.rank: 0.042559584988748886\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'a café'\n",
      "ic| phrase.chunks: [a café]\n",
      "ic| phrase.rank: 0.042559584988748886\n",
      "    phrase.count: 1\n",
      "    phrase.text: 'the UK'\n",
      "ic| phrase.chunks: [the UK]\n",
      "ic| phrase.rank: 0.0, phrase.count: 2, phrase.text: 'She'\n",
      "ic| phrase.chunks: [She, She]\n",
      "ic| phrase.rank: 0.0, phrase.count: 1, phrase.text: 'she'\n",
      "ic| phrase.chunks: [she]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fraces mejor clasificadas em el documento:\n",
    "for phrase in doc._.phrases:\n",
    "    ic(phrase.rank, phrase.count, phrase.text)\n",
    "    ic(phrase.chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4624a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| phrase: Phrase(text='Italian', chunks=[Italian], count=1, rank=0.1760412407399692)\n",
      "ic| phrase: Phrase(text='Piccadilly Circus',\n",
      "                   chunks=[Piccadilly Circus, Piccadilly Circus],\n",
      "                   count=2,\n",
      "                   rank=0.17507452769776916)\n",
      "ic| phrase: Phrase(text='English',\n",
      "                   chunks=[English, English],\n",
      "                   count=2,\n",
      "                   rank=0.1182519975313125)\n",
      "ic| phrase: Phrase(text='Maria', chunks=[Maria, Maria], count=2, rank=0.1182519975313125)\n",
      "ic| phrase: Phrase(text='UK', chunks=[UK], count=1, rank=0.09214420443694846)\n",
      "ic| phrase: Phrase(text='Shawn', chunks=[Shawn], count=1, rank=0.08133885040709285)\n",
      "ic| phrase: Phrase(text='a few years ago',\n",
      "                   chunks=[a few years ago],\n",
      "                   count=1,\n",
      "                   rank=0.06939268322340339)\n",
      "ic| phrase: Phrase(text='Shawn’s girlfriend’s name',\n",
      "                   chunks=[Shawn’s girlfriend’s name],\n",
      "                   count=1,\n",
      "                   rank=0.06094822033701628)\n",
      "ic| phrase: Phrase(text='a café', chunks=[a café], count=1, rank=0.042559584988748886)\n",
      "ic| phrase: Phrase(text='the UK', chunks=[the UK], count=1, rank=0.042559584988748886)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    ic(phrase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fb7ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| phrase: Phrase(text='Italian', chunks=[Italian], count=1, rank=0.1760412407399692)\n",
      "ic| phrase: Phrase(text='Piccadilly Circus',\n",
      "                   chunks=[Piccadilly Circus, Piccadilly Circus],\n",
      "                   count=2,\n",
      "                   rank=0.17507452769776916)\n",
      "ic| phrase: Phrase(text='English',\n",
      "                   chunks=[English, English],\n",
      "                   count=2,\n",
      "                   rank=0.1182519975313125)\n",
      "ic| phrase: Phrase(text='Maria', chunks=[Maria, Maria], count=2, rank=0.1182519975313125)\n",
      "ic| phrase: Phrase(text='UK', chunks=[UK], count=1, rank=0.09214420443694846)\n",
      "ic| phrase: Phrase(text='Shawn', chunks=[Shawn], count=1, rank=0.08133885040709285)\n",
      "ic| phrase: Phrase(text='a few years ago',\n",
      "                   chunks=[a few years ago],\n",
      "                   count=1,\n",
      "                   rank=0.06939268322340339)\n",
      "ic| phrase: Phrase(text='Shawn’s girlfriend’s name',\n",
      "                   chunks=[Shawn’s girlfriend’s name],\n",
      "                   count=1,\n",
      "                   rank=0.06094822033701628)\n",
      "ic| phrase: Phrase(text='a café', chunks=[a café], count=1, rank=0.042559584988748886)\n",
      "ic| phrase: Phrase(text='the UK', chunks=[the UK], count=1, rank=0.042559584988748886)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\", config={ \"stopwords\": { \"word\": [\"NOUN\"] } })\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    ic(phrase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58358fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
